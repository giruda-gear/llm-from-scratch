GPT_CONFIG_124M = {
    "vocab_size": 50257,    # vocabulary size
    "context_length": 1024, # context length
    "emb_dim": 768,         # embedding dimension
    "n_heads": 12,          # number of attention heads
    "n_layers": 12,         # number of layers
    "drop_rate": 0.1,       # dropout rate
    "qkv_bias": False       # query-key-value bias
}